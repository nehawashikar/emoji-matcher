# -*- coding: utf-8 -*-
"""LSTM - Emoji Matcher

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1k1njydrQVHEca8Wr2DFa2qsoK1wuOdw3
"""

print('hi')

from google.colab import files
uploaded = files.upload()

import pandas as pd
#Convert mappings into a dictionary
mapping = pd.read_csv('Mapping.csv')
#print(mapping.columns)
mapping = mapping.drop(columns = ['Unnamed: 0'])

emojis = mapping['emoticons'].to_list()
emoji_number = mapping['number'].to_list()
mapping = dict(zip(emoji_number, emojis))
mapping

"""PREPROCESSING"""

import matplotlib.pyplot as plt
import numpy as np
import re
import nltk  #natural language processing
from nltk.corpus import stopwords
nltk.download("stopwords")
from nltk.stem.porter import *

my_columns = ["Index", "Text", "Label"]

#read into pandas dataframe
sampled = pd.read_csv('Train.csv',
                      skiprows = 1,
                      names=my_columns,
                      usecols = [1,2])

#take random sample of 30,000
df = sampled.sample(n=30000, random_state=1)

df.reset_index(drop=True, inplace=True)
print(df.head())

def clean_text(text):
    #remove tags
    no_tag = re.sub(r'@[A-Za-z0-9]+', '', text)
    #remove numbers
    no_num = re.sub(r'[0-9]+', '', no_tag)
    #remove punctuation
    no_punc = re.sub(r'[^\w\s]', '', no_num)
    #lower
    lowercase = no_punc.lower()
    #split based on whitespace
    splitted = lowercase.split()
    #remove stop words
    no_stopwords = [w for w in splitted if w not in stopwords.words("english")]
    #get base form of words
    porter = PorterStemmer()
    stemmed = [porter.stem(w) for w in no_stopwords]
    return stemmed

df['Text'] = df['Text'].apply(clean_text)

df

#get statistics on data
from collections import Counter

print(type(df['Text'][0]))
print(df['Text'][0])
all_words = [word for entry in df['Text']for word in entry]
print("Number of words:", len(all_words))
unique_words = set(all_words)
print("Number unique words:", len(unique_words))

word_freq = Counter(all_words)

top_words = dict(word_freq.most_common(30000))
top_word_freq = sum(top_words.values())
total_word_freq = sum(word_freq.values())
print(f"Frequency for top 30000 words: {top_word_freq/total_word_freq:.2%}")


lengths = df['Text'].apply(lambda x: len(x))
print("99 percent", np.percentile(lengths, 99))

print("mean", lengths.mean())

#check balance of data
print(df.info())
print(df.head())
df['Label'].value_counts()

import matplotlib.pyplot as plt
import seaborn as sns

#get the ditribution
distr = df['Label'].value_counts()

# Map the index to emojis
distr.index = distr.index.map(mapping)
print("Emoji Percentages:", distr)

#print bar plot of distributions
plt.figure(figsize=(12, 8))
sns.barplot(x=distr.index,
            y=distr.values,
            palette='viridis')
plt.title('Class Distribution in Dataset (Emojis)')
plt.xlabel('Labels (Emojis)')
plt.ylabel('Percentage')
plt.show()

trainX = df['Text'].values
trainY = df['Label'].values

trainX.shape, trainY.shape

embedding_index = {}

with open('glove.6B.50d.txt', 'r') as glove:
  for line in glove:
      #split every line by whitespace to get each embedding value
      values = line.split()
      #get embedding's word
      word = values[0]
      #map embeddings to word in a dict
      emb = np.array(values[1:], dtype ='float')
      embedding_index[word] = emb

def get_embedding_output(X):
    maxLen = 20
    embedding_dim = 50
    embedding_output = np.zeros((len(X), maxLen, embedding_dim))

    for ix in range(len(X)):
        my_example = X[ix]
        for ij in range(min(len(my_example), maxLen)):
            word_lower = my_example[ij].lower()
            if embedding_index.get(word_lower) is not None:
                embedding_output[ix][ij] = embedding_index[word_lower]

    return embedding_output

trainX_glove = get_embedding_output(trainX)
trainY = df['Label'].values
trainX_glove.shape, trainY.shape

from keras.utils import to_categorical
trainY = to_categorical(trainY)
print(trainY.shape)
trainY[1]

#SMOTE for oversampling
from imblearn.over_sampling import SMOTE
trainX_glove = trainX_glove.reshape(len(trainX_glove), -1)

oversample = SMOTE()
X, y = oversample.fit_resample(trainX_glove, trainY)

# get the new distribution of the labels
counts = np.sum(y, axis=0)
label_distribution = pd.Series(counts, index=[f"{i}" for i in range(len(counts))])
print(label_distribution)

X = X.reshape(-1, 20, 50)
X.shape, y.shape

#split data into training and testing
from sklearn.model_selection import train_test_split
trainX, testX, trainY, testY = train_test_split(X, y, test_size=0.2, random_state=42)

testY

from keras.models import Sequential
from keras.layers import LSTM, Dense, Dropout, Bidirectional

model = Sequential([
    Bidirectional(LSTM(128, return_sequences=True), input_shape = (20,50)),
    Dropout(0.3),
    Bidirectional(LSTM(units=64)),
    Dropout(0.3),
    Dense(128, activation='relu'),
    Dense(64, activation='relu'),
    Dense(20, activation='softmax')
])
model.summary()

model.compile(optimizer='adam', loss='categorical_crossentropy', metrics =['accuracy'])

hist = model.fit(trainX, trainY, validation_split=0.2, shuffle=True, batch_size=64, epochs=30)

train_loss = hist.history['loss']
val_loss = hist.history['val_loss']

import matplotlib.pyplot as plt

epochs = range(1, len(train_loss) + 1)

plt.figure(figsize=(10, 6))
plt.plot(epochs, train_loss, 'bo-', label='Training Loss')
plt.plot(epochs, val_loss, 'ro-', label='Validation Loss')
plt.title('Training and Validation Loss')
plt.xlabel('Epoch')
plt.ylabel('Log Loss')
plt.legend()

plt.show()

train_acc = hist.history['accuracy']
val_acc = hist.history['val_accuracy']

import matplotlib.pyplot as plt

epochs = range(1, len(train_acc) + 1)

plt.figure(figsize=(10, 6))
plt.plot(epochs, train_acc, 'bo-', label='Training Accuracy')
plt.plot(epochs, val_acc, 'ro-', label='Validation Accuracy')
plt.title('Training and Validation Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()

plt.show()

model.evaluate(testX, testY)

predictions = model.predict(testX)
print(predictions)

predicted_classes = np.argmax(predictions, axis=1)
print(predicted_classes)

message = "Thank you for listening to our presentation"
words = message.split()
words = [word.lower() for word in words]
input_data = get_embedding_output([words])
predicted_emoji = model.predict(input_data)
predicted_class = np.argmax(predicted_emoji, axis=1)
print(message, ":", mapping[predicted_class[0]])

print(testY)

actual_classes = np.argmax(testY, axis=1)
actual_classes

from sklearn.metrics import f1_score, recall_score, precision_score

precision = precision_score(actual_classes, predicted_classes, average='macro')
recall = recall_score(actual_classes, predicted_classes, average='macro')
f1 = f1_score(actual_classes, predicted_classes, average='macro')

print(f"Precision: {precision}")
print(f"Recall: {recall}")
print(f"F1 Score: {f1}")

actual_emojis = [mapping[label] for label in actual_classes]
predicted_emojis = [mapping[label] for label in predicted_classes]

df2 = pd.read_csv('Train.csv',
                 skiprows = 1,
                 names=my_columns,
                 index_col=0)

texts = df2['Text'].tolist()

for i in range(0, len(actual_emojis), 8):
    print(f"Text: {texts[i]}, Actual: {actual_emojis[i]}, Predicted: {predicted_emojis[i]}")

hearts=0
mislabeled = 0
same_heart= 0
print("Total number of predictions:", len(actual_emojis))
for i in range(0, len(actual_emojis)):
  if(actual_classes[i] in [9, 14, 15, 18]):
    hearts = hearts +1
  if((actual_classes[i] in [9, 14, 15, 18]) and (predicted_classes[i] in [9, 14, 15, 18])):
    if(actual_classes[i] != predicted_classes[i]):
      mislabeled = mislabeled+1
    if(actual_classes[i] == predicted_classes[i]):
      same_heart = same_heart+1

print("Total number of actual hearts:", hearts)
print("Total number of correctly labeled hearts:", same_heart)
print("Total number of mislabeled hearts:", mislabeled)
